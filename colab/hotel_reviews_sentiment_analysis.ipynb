{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakkapongz/hotel-reviews-sentiment-analysis/blob/develop/colab/hotel_reviews_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBuJqfdH-BTQ"
      },
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpcnIXLC-Tpk"
      },
      "source": [
        "## Install Libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBKbJgj6P_8v"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchtext -y\n",
        "!pip uninstall torchaudio torchvision -y\n",
        "!pip uninstall fastai torchmetrics -y\n",
        "!pip -q install torch==1.5.0 torchtext==0.4.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
        "!pip -q install pythainlp wordcloud thai2transformers\n",
        "\n",
        "!pip list\n",
        "\n",
        "!pip freeze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egUXehRfAqGT"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bc1UKxhMQEtC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "# SET PATH TO DATA FOLDER\n",
        "path= \"/content/drive/Shareddrives/EGIT697_THEMATIC\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2uGF35ifrMC"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2h5GMWatneN"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26X8Rh4J8ht6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "all_tsv_df = pd.read_csv(path + '/7500_good_longest_token_more_than_8_03082023.tsv', delimiter='\\t')\n",
        "\n",
        "all_tsv_df['label'].value_counts().plot.bar()\n",
        "\n",
        "all_tsv_df\n",
        "\n",
        "# all_df = pd.read_pickle(path + '/10000_good_bad_reviews_no_gap_02012023.pkl')\n",
        "\n",
        "# all_df['label'].value_counts().plot.bar()\n",
        "\n",
        "# all_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiYIVknLuXTj"
      },
      "source": [
        "# **Feature Extraction Method**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdgEwPLrjV_D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import gc\n",
        "import copy\n",
        "\n",
        "# check GPU available?\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQiUwCBnAV3F"
      },
      "source": [
        "## **Wangchanberta** (Monolingual Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULAtZyI2UVZR"
      },
      "outputs": [],
      "source": [
        "from transformers import (AutoTokenizer, AutoModel, pipeline, AutoModelForSequenceClassification)\n",
        "\n",
        "# active GPU\n",
        "# device = torch.device(\"cuda\")\n",
        "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# model loading\n",
        "wangchan_tokenizer = AutoTokenizer.from_pretrained(f'airesearch/wangchanberta-base-att-spm-uncased', output_hidden_states=True)\n",
        "wangchan_model = AutoModel.from_pretrained(f'airesearch/wangchanberta-base-att-spm-uncased', output_hidden_states=True)\n",
        "wangchan_model = wangchan_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQw7u5ba18d9"
      },
      "outputs": [],
      "source": [
        "def adjust_encoded_input_wangchan(encoded_input):\n",
        "\n",
        "  # delete first and last separator token and splits to 414 tokens\n",
        "  input_ids_chunks = list(encoded_input['input_ids'][0][1:-1].split(414))\n",
        "  attention_mask_chunks = list(encoded_input['attention_mask'][0][1:-1].split(414))\n",
        "\n",
        "  for i in range(len(input_ids_chunks)):\n",
        "\n",
        "    # add 5 to the first and 6 to last element tonsor padding len to 512 for transformer model \n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], torch.Tensor([6, 5]).long() ])\n",
        "    # shifting 6 5\n",
        "    input_ids_chunks[i] = torch.roll(input_ids_chunks[i], 1, 0)\n",
        "    \n",
        "    # padding len to 416 for transformer model\n",
        "    pad_len = 416 - input_ids_chunks[i].shape[0]\n",
        "\n",
        "    # 1 token represents padding <pad>\n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], torch.Tensor([1] * pad_len).long()])\n",
        "\n",
        "    if len(attention_mask_chunks[i]) == 414:\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], torch.Tensor([1] * 2).long()])\n",
        "    else:\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], torch.Tensor([1] * 2).long()])\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], torch.Tensor([0] * (pad_len)).long()])\n",
        "\n",
        "  input_ids = torch.stack(input_ids_chunks)\n",
        "  attention_mask = torch.stack(attention_mask_chunks)\n",
        "\n",
        "  input_dict = {\n",
        "      'input_ids': input_ids.long(),\n",
        "      'attention_mask': attention_mask.int()\n",
        "  }\n",
        "\n",
        "  return input_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40LGiMByiHwa"
      },
      "source": [
        "adjust_encoded_input_wangchan method ที่ช่วยจัดการ split word tokens ที่ยาวเกินให้มีขนาด 416 ส่วนที่เหลือจะถูกทำการ padding ด้วย 1 ในฝั่งของ wangchanberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NywcwbEDOQC6"
      },
      "outputs": [],
      "source": [
        "# EXAMPLE\n",
        "text = \"จากก้าวแรกที่ผ่านประตูทางเข้า ด้วยกลิ่นหอมของเครื่องหอมที่จัดไว้ของทางโรงแรมทำให้เกิดความรู้สึกของการต้อนรับที่อบอุ่นของเจ้าของโรงแรม ที่ไม่ได้เกิดจากบุคคล ผสมผสานกับการเอาใจใส่ของพนักงานที่ได้รับการฝึกอบรมมาอย่างดีทำให้รับรู้ได้ถึงคำว่าบริการ 5 ดาวในโรงแรมขนาดเล็ก ห้องพักที่เข้าพักเป็นห้องชั้นล่าง สำหรับตึก 2 ชั้นใกล้สระว่ายน้ำขนาดเล็ก ทำให้สะดวกในการเล่นน้ำในสระ ประกอบกับการออกแบบในสไตล์ Indochina ที่พบเห็นไม่บ่อยครั้งนัก ทำให้การพักผ่อนในครั้งนี้ นับเป็นการพักผ่อนที่สมบูรณ์แบบอีกครั้งหนึ่ง\"\n",
        "\n",
        "encoded_input = wangchan_tokenizer(text, return_tensors='pt').to(device)\n",
        "encoded_input = adjust_encoded_input_wangchan(encoded_input)\n",
        "encoded_input['input_ids'], encoded_input['input_ids'].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a3VhB_lnHHI"
      },
      "outputs": [],
      "source": [
        "def extract_last_four_with_wangchan(input_text, feature_extractor):\n",
        "\n",
        "  encoded_input = wangchan_tokenizer(input_text, return_tensors='pt').to(device)\n",
        "  encoded_input = adjust_encoded_input_wangchan(encoded_input)\n",
        "  _, _, hidden_states = feature_extractor(**encoded_input)\n",
        "\n",
        "  # use only last 4 layers \n",
        "  last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
        "\n",
        "  # concat last 4 layers vectors then calculate mean between vectors\n",
        "  cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
        "  cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
        "\n",
        "  # if document only has 1 batch, no need to sum vector\n",
        "  if cat_sentence_embedding.shape[0] != 3072:\n",
        "\n",
        "    doc_embedding = torch.sum(cat_sentence_embedding, dim=0)\n",
        "  else:\n",
        "    doc_embedding = copy.copy(cat_sentence_embedding)\n",
        "  \n",
        "  return doc_embedding.cpu().detach().numpy().astype('float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCkfGlQhw4vF"
      },
      "source": [
        "We can use the outputs of WangchanBERTa (or any transformer-based models) as document vectors as an example by [BramVanroy](https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb).\n",
        "\n",
        "![](https://github.com/BramVanroy/bert-for-inference/raw/ab7c57d6e7c79faf83ac0f9b6595c4b3d660c43c/img/bert-feature-extraction-contextualized-embeddings.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg1787x2IMIB"
      },
      "outputs": [],
      "source": [
        "text = \"จากก้าวแรกที่ผ่านประตูทางเข้า ด้วยกลิ่นหอมของเครื่องหอมที่จัดไว้ของทางโรงแรมทำให้เกิดความรู้สึกของการต้อนรับที่อบอุ่นของเจ้าของโรงแรม ที่ไม่ได้เกิดจากบุคคล ผสมผสานกับการเอาใจใส่ของพนักงานที่ได้รับการฝึกอบรมมาอย่างดีทำให้รับรู้ได้ถึงคำว่าบริการ 5 ดาวในโรงแรมขนาดเล็ก ห้องพักที่เข้าพักเป็นห้องชั้นล่าง สำหรับตึก 2 ชั้นใกล้สระว่ายน้ำขนาดเล็ก ทำให้สะดวกในการเล่นน้ำในสระ ประกอบกับการออกแบบในสไตล์ Indochina ที่พบเห็นไม่บ่อยครั้งนัก ทำให้การพักผ่อนในครั้งนี้ นับเป็นการพักผ่อนที่สมบูรณ์แบบอีกครั้งหนึ่ง\"\n",
        "\n",
        "print(wangchan_model)\n",
        "\n",
        "# t_2 = extract_last_four_with_wangchan(text, wangchan_model)\n",
        "# t_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfONTRQwhqS-"
      },
      "outputs": [],
      "source": [
        "len(t_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IpQDo0X_2zg"
      },
      "source": [
        "## **Bert Model for Multilingual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BF50pe294ah"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "# active GPU\n",
        "# device = torch.device(\"cuda\")\n",
        "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', output_hidden_states=True)\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\", output_hidden_states=True)\n",
        "bert_model = bert_model.to(device)\n",
        "# bert_class = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5aQatvV96uz"
      },
      "outputs": [],
      "source": [
        "a = torch.Tensor([102, 101]).long()\n",
        "c = torch.Tensor([1] * 2).long()\n",
        "def adjust_encoded_input(encoded_input):\n",
        "\n",
        "  # delete first and last separator token and splits to 510 tokens\n",
        "  input_ids_chunks = list(encoded_input['input_ids'][0][1:-1].split(510))\n",
        "  attention_mask_chunks = list(encoded_input['attention_mask'][0][1:-1].split(510))\n",
        "\n",
        "  for i in range(len(input_ids_chunks)):\n",
        "\n",
        "    # add 101 to the first and 102 to last element tonsor padding len to 512 for transformer model \n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], a ])\n",
        "    # shifting 101 102\n",
        "    input_ids_chunks[i] = torch.roll(input_ids_chunks[i], 1, 0)\n",
        "    \n",
        "    # padding len to 512 for transformer model\n",
        "    pad_len = 512 - input_ids_chunks[i].shape[0]\n",
        "    b = torch.Tensor([0] * pad_len).long()\n",
        "\n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], b])\n",
        "\n",
        "    if len(attention_mask_chunks[i]) == 510:\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "    else:\n",
        "      d = torch.Tensor([0] * (pad_len)).long()\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], d])\n",
        "\n",
        "  input_ids = torch.stack(input_ids_chunks)\n",
        "  attention_mask = torch.stack(attention_mask_chunks)\n",
        "\n",
        "  input_dict = {\n",
        "      'input_ids': input_ids.long(),\n",
        "      'attention_mask': attention_mask.int()\n",
        "  }\n",
        "\n",
        "  return input_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq9FTESKnv2n"
      },
      "source": [
        "method ที่ช่วยจัดการ split word tokens ที่ยาวเกินให้มีขนาด 512 ส่วนที่เหลือจะถูกทำการ padding ด้วย 0 ในฝั่งของ bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxKyydB4OzGa"
      },
      "outputs": [],
      "source": [
        "text = \"จากก้าวแรกที่ผ่านประตูทางเข้า ด้วยกลิ่นหอมของเครื่องหอมที่จัดไว้ของทางโรงแรมทำให้เกิดความรู้สึกของการต้อนรับที่อบอุ่นของเจ้าของโรงแรม ที่ไม่ได้เกิดจากบุคคล ผสมผสานกับการเอาใจใส่ของพนักงานที่ได้รับการฝึกอบรมมาอย่างดีทำให้รับรู้ได้ถึงคำว่าบริการ 5 ดาวในโรงแรมขนาดเล็ก ห้องพักที่เข้าพักเป็นห้องชั้นล่าง สำหรับตึก 2 ชั้นใกล้สระว่ายน้ำขนาดเล็ก ทำให้สะดวกในการเล่นน้ำในสระ ประกอบกับการออกแบบในสไตล์ Indochina ที่พบเห็นไม่บ่อยครั้งนัก ทำให้การพักผ่อนในครั้งนี้ นับเป็นการพักผ่อนที่สมบูรณ์แบบอีกครั้งหนึ่ง\"\n",
        "\n",
        "encoded_input = bert_tokenizer(text, return_tensors='pt').to(device)\n",
        "encoded_input = adjust_encoded_input(encoded_input)\n",
        "encoded_input['input_ids'], encoded_input['input_ids'].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uObhujrY98cQ"
      },
      "outputs": [],
      "source": [
        "def extract_last_four_with_bert(input_text, feature_extractor):\n",
        "  encoded_input = bert_tokenizer(input_text, return_tensors='pt').to(device)\n",
        "  encoded_input = adjust_encoded_input_wangchan(encoded_input)\n",
        "  # hidden_states = feature_extractor(**encoded_input)[0]\n",
        "  _, _, hidden_states = feature_extractor(**encoded_input)\n",
        "\n",
        "  # FOR MEAN CALCULATION BETWEEN TENSOR DIMENSION\n",
        "\n",
        "  last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
        "\n",
        "  cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
        "  cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
        "\n",
        "  if cat_sentence_embedding.shape[0] != 3072:\n",
        "\n",
        "    doc_embedding = torch.sum(cat_sentence_embedding, dim=0)\n",
        "  else:\n",
        "    doc_embedding = copy.copy(cat_sentence_embedding)\n",
        "\n",
        "  return doc_embedding.cpu().detach().numpy().astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiA9K72JnQtJ"
      },
      "outputs": [],
      "source": [
        "text = \"จากก้าวแรกที่ผ่านประตูทางเข้า ด้วยกลิ่นหอมของเครื่องหอมที่จัดไว้ของทางโรงแรมทำให้เกิดความรู้สึกของการต้อนรับที่อบอุ่นของเจ้าของโรงแรม ที่ไม่ได้เกิดจากบุคคล ผสมผสานกับการเอาใจใส่ของพนักงานที่ได้รับการฝึกอบรมมาอย่างดีทำให้รับรู้ได้ถึงคำว่าบริการ 5 ดาวในโรงแรมขนาดเล็ก ห้องพักที่เข้าพักเป็นห้องชั้นล่าง สำหรับตึก 2 ชั้นใกล้สระว่ายน้ำขนาดเล็ก ทำให้สะดวกในการเล่นน้ำในสระ ประกอบกับการออกแบบในสไตล์ Indochina ที่พบเห็นไม่บ่อยครั้งนัก ทำให้การพักผ่อนในครั้งนี้ นับเป็นการพักผ่อนที่สมบูรณ์แบบอีกครั้งหนึ่ง\"\n",
        "\n",
        "t_2 = extract_last_four_with_bert(text, bert_model)\n",
        "t_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UoioYEhb4dY"
      },
      "outputs": [],
      "source": [
        "len(t_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuceVeO09_L3"
      },
      "source": [
        "## **XLM-RoberTa Model for Multilingual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJNdzp-L-Mvk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, XLMRobertaModel\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", output_hidden_states=True)\n",
        "xlmr_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\", output_hidden_states=True)\n",
        "xlmr_model = xlmr_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsxLV-xUDbgh"
      },
      "outputs": [],
      "source": [
        "a = torch.Tensor([102, 101]).long()\n",
        "c = torch.Tensor([1] * 2).long()\n",
        "def adjust_encoded_input_xlmr(encoded_input):\n",
        "\n",
        "  # delete first and last separator token and splits to 510 tokens\n",
        "  input_ids_chunks = list(encoded_input['input_ids'][0][1:-1].split(510))\n",
        "  attention_mask_chunks = list(encoded_input['attention_mask'][0][1:-1].split(510))\n",
        "\n",
        "  for i in range(len(input_ids_chunks)):\n",
        "\n",
        "    # add 101 to the first and 102 to last element tonsor padding len to 512 for transformer model \n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], a ])\n",
        "    # shifting 101 102\n",
        "    input_ids_chunks[i] = torch.roll(input_ids_chunks[i], 1, 0)\n",
        "    \n",
        "    # padding len to 512 for transformer model\n",
        "    pad_len = 512 - input_ids_chunks[i].shape[0]\n",
        "    b = torch.Tensor([0] * pad_len).long()\n",
        "\n",
        "    input_ids_chunks[i] = torch.cat([input_ids_chunks[i], b])\n",
        "\n",
        "    if len(attention_mask_chunks[i]) == 510:\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "    else:\n",
        "      d = torch.Tensor([0] * (pad_len)).long()\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], c])\n",
        "      attention_mask_chunks[i] = torch.cat([attention_mask_chunks[i], d])\n",
        "\n",
        "  input_ids = torch.stack(input_ids_chunks)\n",
        "  attention_mask = torch.stack(attention_mask_chunks)\n",
        "\n",
        "  input_dict = {\n",
        "      'input_ids': input_ids.long(),\n",
        "      'attention_mask': attention_mask.int()\n",
        "  }\n",
        "\n",
        "  return input_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H1OoCS5Dmjd"
      },
      "outputs": [],
      "source": [
        "# EXAMPLE\n",
        "text = \"จากก้าวแรกที่ผ่านประตูทางเข้า ด้วยกลิ่นหอมของเครื่องหอมที่จัดไว้ของทางโรงแรมทำให้เกิดความรู้สึกของการต้อนรับที่อบอุ่นของเจ้าของโรงแรม ที่ไม่ได้เกิดจากบุคคล ผสมผสานกับการเอาใจใส่ของพนักงานที่ได้รับการฝึกอบรมมาอย่างดีทำให้รับรู้ได้ถึงคำว่าบริการ 5 ดาวในโรงแรมขนาดเล็ก ห้องพักที่เข้าพักเป็นห้องชั้นล่าง สำหรับตึก 2 ชั้นใกล้สระว่ายน้ำขนาดเล็ก ทำให้สะดวกในการเล่นน้ำในสระ ประกอบกับการออกแบบในสไตล์ Indochina ที่พบเห็นไม่บ่อยครั้งนัก ทำให้การพักผ่อนในครั้งนี้ นับเป็นการพักผ่อนที่สมบูรณ์แบบอีกครั้งหนึ่ง\"\n",
        "\n",
        "encoded_input = xlmr_tokenizer(text, return_tensors='pt').to(device)\n",
        "encoded_input = adjust_encoded_input_xlmr(encoded_input)\n",
        "encoded_input['input_ids'], encoded_input['input_ids'].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXatHVuWERVM"
      },
      "outputs": [],
      "source": [
        "def extract_last_four_with_xlmr(input_text, feature_extractor):\n",
        "  encoded_input = xlmr_tokenizer(input_text, return_tensors='pt').to(device)\n",
        "  encoded_input = adjust_encoded_input_wangchan(encoded_input)\n",
        "  # hidden_states = feature_extractor(**encoded_input)[0]\n",
        "  _, _, hidden_states = feature_extractor(**encoded_input)\n",
        "\n",
        "  # FOR MEAN CALCULATION BETWEEN TENSOR DIMENSION\n",
        "\n",
        "  last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
        "\n",
        "  cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
        "  cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
        "\n",
        "  if cat_sentence_embedding.shape[0] != 3072:\n",
        "\n",
        "    doc_embedding = torch.sum(cat_sentence_embedding, dim=0)\n",
        "  else:\n",
        "    doc_embedding = copy.copy(cat_sentence_embedding)\n",
        "\n",
        "  return doc_embedding.cpu().detach().numpy().astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKQRSE1UEa8U"
      },
      "outputs": [],
      "source": [
        "text = \"จากก้าวแรกที่ผ่านประตูทางเข้า ด้วยกลิ่นหอมของเครื่องหอมที่จัดไว้ของทางโรงแรมทำให้เกิดความรู้สึกของการต้อนรับที่อบอุ่นของเจ้าของโรงแรม ที่ไม่ได้เกิดจากบุคคล ผสมผสานกับการเอาใจใส่ของพนักงานที่ได้รับการฝึกอบรมมาอย่างดีทำให้รับรู้ได้ถึงคำว่าบริการ 5 ดาวในโรงแรมขนาดเล็ก ห้องพักที่เข้าพักเป็นห้องชั้นล่าง สำหรับตึก 2 ชั้นใกล้สระว่ายน้ำขนาดเล็ก ทำให้สะดวกในการเล่นน้ำในสระ ประกอบกับการออกแบบในสไตล์ Indochina ที่พบเห็นไม่บ่อยครั้งนัก ทำให้การพักผ่อนในครั้งนี้ นับเป็นการพักผ่อนที่สมบูรณ์แบบอีกครั้งหนึ่ง\"\n",
        "\n",
        "t_2 = extract_last_four_with_xlmr(text, xlmr_model)\n",
        "t_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR-LgApfM_bq"
      },
      "outputs": [],
      "source": [
        "len(t_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjZLd2PSBVd1"
      },
      "source": [
        "## Extract last four from data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gil6nWbhdNtT"
      },
      "outputs": [],
      "source": [
        "wangchan_vectors = []\n",
        "bert_vectors = []\n",
        "xlmr_vectors = []\n",
        "\n",
        "for idx, row in all_tsv_df.iterrows():\n",
        "  text = row['review_text']\n",
        "  wangchan_vector = extract_last_four_with_wangchan(text, wangchan_model)\n",
        "  bert_vector = extract_last_four_with_bert(text, bert_model)\n",
        "  xlmr_vector = extract_last_four_with_xlmr(text, xlmr_model)\n",
        "  wangchan_vectors.append(wangchan_vector)\n",
        "  bert_vectors.append(bert_vector)\n",
        "  xlmr_vectors.append(xlmr_vector)\n",
        "  print(idx)\n",
        "\n",
        "all_tsv_df['content_bert_vector'] = bert_vectors\n",
        "all_tsv_df['content_wangchan_vector'] = wangchan_vectors\n",
        "all_tsv_df['content_xlmr_vector'] = xlmr_vectors\n",
        "\n",
        "all_tsv_df.to_pickle(path + \"/7500_good_longest_token_more_than_8_03082023.tsv.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oD-P-zJZosw"
      },
      "outputs": [],
      "source": [
        "pos_df = pd.read_pickle(path + '/5000_good_reviews_no_gap_02012023.pkl')\n",
        "\n",
        "pos_df['label'].value_counts().plot.bar()\n",
        "\n",
        "pos_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vageFQrjVEel"
      },
      "outputs": [],
      "source": [
        "neg_df = pd.read_pickle(path + '/5000_bad_reviews_no_gap_02012023.pkl')\n",
        "\n",
        "neg_df['label'].value_counts().plot.bar()\n",
        "\n",
        "neg_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crHWtx-cZzBc"
      },
      "outputs": [],
      "source": [
        "combined_df = neg_df.append(pos_df, ignore_index=True)\n",
        "\n",
        "# # # combined_df = pos_df.append(neg_df, ignore_index=True)\n",
        "\n",
        "combined_df.rename(columns = {'review_token':'review_tokens'}, inplace = True)\n",
        "combined_df\n",
        "\n",
        "combined_df.to_pickle(path + \"/10000_good_bad_reviews_no_gap_02012023.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFwZwZXGaVAD"
      },
      "outputs": [],
      "source": [
        "# SAMPLE GOOD TEXT WANGCHAN\n",
        "text = all_df.iloc[5]['review_text']\n",
        "\n",
        "print(text + '\\n')\n",
        "\n",
        "encoded_input = wangchan_tokenizer(text, return_tensors='pt').to(device)\n",
        "encoded_input = adjust_encoded_input_wangchan(encoded_input)\n",
        "encoded_input['input_ids'], encoded_input['input_ids'].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUSI-f9DcZI5"
      },
      "outputs": [],
      "source": [
        "# SAMPLE BAD TEXT WANGCHAN\n",
        "text = all_df.iloc[5002]['review_text']\n",
        "\n",
        "print(text + '\\n')\n",
        "\n",
        "encoded_input = wangchan_tokenizer(text, return_tensors='pt').to(device)\n",
        "encoded_input = adjust_encoded_input_wangchan(encoded_input)\n",
        "encoded_input['input_ids'], encoded_input['input_ids'].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfDFu73wNoMO"
      },
      "source": [
        "## PyThai NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgSdIaJKGvGp"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI_1TaZHHKnZ"
      },
      "source": [
        "####Light Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNvb9uSQNlPV"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from pythainlp import word_tokenize\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "from pythainlp import sent_tokenize, word_tokenize\n",
        "\n",
        "thai_stopwords = list(thai_stopwords())\n",
        "\n",
        "def text_process(text):\n",
        "\n",
        "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', \"ๆ\", \"ฯ\"))\n",
        "    final = final.translate(str.maketrans('','', string.punctuation))\n",
        "\n",
        "    final = word_tokenize(final)\n",
        "    final = \" \".join(word for word in final)\n",
        "    return final\n",
        "\n",
        "all_df['review_tokens'] = all_df['review_text'].apply(text_process)\n",
        "# all_df.head()\n",
        "\n",
        "X = all_df[['review_tokens']]\n",
        "y = all_df['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5hDvHK2HT3M"
      },
      "source": [
        "####Deep Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzHGoEmAGs4v"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from pythainlp import word_tokenize\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "from pythainlp import sent_tokenize, word_tokenize\n",
        "from thai2transformers import preprocess\n",
        "\n",
        "thai_stopwords = list(thai_stopwords())\n",
        "\n",
        "def text_process(text):\n",
        "\n",
        "    t = preprocess.fix_html(text)\n",
        "    t = preprocess.rm_brackets(t)\n",
        "    t = preprocess.replace_newlines(t)\n",
        "    t = preprocess.rm_useless_spaces(t)\n",
        "    t = preprocess.replace_spaces(t)\n",
        "    t = preprocess.replace_rep_after(t)\n",
        "\n",
        "    tokens = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', \"ๆ\", \"ฯ\"))\n",
        "    tokens = tokens.translate(str.maketrans('','', string.punctuation))\n",
        "\n",
        "    tokens = word_tokenize(tokens)\n",
        "    tokens = preprocess.ungroup_emoji(tokens)\n",
        "    tokens = preprocess.replace_wrep_post(tokens)\n",
        "\n",
        "    final = \" \".join(word for word in tokens)\n",
        "\n",
        "    return final\n",
        "\n",
        "all_df['deep_clean_review_tokens'] = all_df['review_text'].apply(text_process)\n",
        "all_df.head()\n",
        "\n",
        "deep_clean_X = all_df[['deep_clean_review_tokens']]\n",
        "deep_clean_y = all_df['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPlkY2tkCc46"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7jyDQUoCoPW"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer=lambda x:x.split(' '))\n",
        "\n",
        "tfidf_vec = tfidf_vectorizer.fit_transform(X['review_tokens'])\n",
        "tfidf_array = np.array(tfidf_vec.todense())\n",
        "\n",
        "content_tfidf = []\n",
        "\n",
        "for vec in tfidf_array:\n",
        "  content_tfidf.append(vec)\n",
        "\n",
        "all_df['content_tfidf_vector'] = content_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfmsSQClMdB-"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer=lambda x:x.split(' '))\n",
        "\n",
        "tfidf_vec = tfidf_vectorizer.fit_transform(deep_clean_X['deep_clean_review_tokens'])\n",
        "tfidf_array = np.array(tfidf_vec.todense())\n",
        "\n",
        "content_tfidf = []\n",
        "\n",
        "for vec in tfidf_array:\n",
        "  content_tfidf.append(vec)\n",
        "\n",
        "all_df['content_deep_clean_tfidf_vector'] = content_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv0Ktm_ZCh3Y"
      },
      "source": [
        "### Bag Of Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa_QnP7vO3s_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer(analyzer=lambda x:x.split(' '))\n",
        "\n",
        "bow_vec = count_vectorizer.fit_transform(X['review_tokens'])\n",
        "bow_array = np.array(bow_vec.todense())\n",
        "\n",
        "content_bow = []\n",
        "\n",
        "for vec in bow_array:\n",
        "  content_bow.append(vec)\n",
        "\n",
        "all_df['content_bow_vector'] = content_bow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0-32UylMoQ6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer(analyzer=lambda x:x.split(' '))\n",
        "\n",
        "bow_vec = count_vectorizer.fit_transform(deep_clean_X['deep_clean_review_tokens'])\n",
        "bow_array = np.array(bow_vec.todense())\n",
        "\n",
        "content_bow = []\n",
        "\n",
        "for vec in bow_array:\n",
        "  content_bow.append(vec)\n",
        "\n",
        "all_df['content_deep_clean_bow_vector'] = content_bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI7wMlrBNHI7"
      },
      "outputs": [],
      "source": [
        "all_df.to_pickle(path + \"/10000_good_bad_reviews_no_gap_02012023_completed.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YS3l2P0bpT9"
      },
      "source": [
        "# Word Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm6NhooH8g6D"
      },
      "source": [
        "## Good words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff0ncsrkbyoq"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "\n",
        "thai_stopwords = list(thai_stopwords())\n",
        "\n",
        "\n",
        "df_pos = all_df[all_df['label'] == 'GOOD']\n",
        "pos_word_all = \" \".join(text for text in df_pos['deep_clean_review_tokens'])\n",
        "# print(pos_word_all)\n",
        "reg = r\"[ก-๙a-zA-Z']+\"\n",
        "fp =  path + '/THSarabunNew.ttf'\n",
        "wordcloud = WordCloud(stopwords=thai_stopwords, background_color = 'white', max_words=2000, height = 2000, width=4000, font_path=fp, regexp=reg).generate(pos_word_all)\n",
        "plt.figure(figsize = (16,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U_S-00A8lCn"
      },
      "source": [
        "## Bad words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHgzjD738pCM"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "\n",
        "thai_stopwords = list(thai_stopwords())\n",
        "\n",
        "\n",
        "df_neg = all_df[all_df['label'] == 'BAD']\n",
        "neg_word_all = \" \".join(text for text in df_neg['deep_clean_review_tokens'])\n",
        "# print(pos_word_all)\n",
        "reg = r\"[ก-๙a-zA-Z']+\"\n",
        "fp =  path + '/THSarabunNew.ttf'\n",
        "wordcloud = WordCloud(stopwords=thai_stopwords, background_color = 'white', max_words=2000, height = 2000, width=4000, font_path=fp, regexp=reg).generate(neg_word_all)\n",
        "plt.figure(figsize = (16,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJl4LDEhY_KR"
      },
      "source": [
        "# Predictive Model Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFLqpxc8aGAJ"
      },
      "outputs": [],
      "source": [
        "all_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJZzsPwOY_9-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support , classification_report\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44ep6lzdZJG2"
      },
      "outputs": [],
      "source": [
        "ks_selected_df = all_df\n",
        "\n",
        "# train test spilt 70/30 ratio\n",
        "ks_df_train, ks_df_test, ks_df_y_train, ks_df_y_test = train_test_split(ks_selected_df, list(ks_selected_df['label']), test_size=0.3, random_state=0)\n",
        "\n",
        "task = {\n",
        "    \"ks_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_bert_vector',\n",
        "        'language_model' : 'BERT (multilingual)'\n",
        "    },\n",
        "    \"kh_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_wangchan_vector',\n",
        "        'language_model' : 'WangchanBERTa (monolingual)'\n",
        "    },\n",
        "    \"xlmr_df\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_xlmr_vector',\n",
        "        'language_model' : 'XML-RoBERTa (multilingual)'\n",
        "    },\n",
        "    \"tfidf\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_tfidf_vector',\n",
        "        'language_model' : 'TF-IDF'\n",
        "    },\n",
        "    \"bow\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_bow_vector',\n",
        "        'language_model' : 'Bag Of Word'\n",
        "    },\n",
        "    \"deep_clean_tfidf\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_deep_clean_tfidf_vector',\n",
        "        'language_model' : 'TF-IDF (Deep clean)'\n",
        "    },\n",
        "    \"bow\": {\n",
        "        'data': ks_df_train,\n",
        "        'col': 'content_deep_clean_bow_vector',\n",
        "        'language_model' : 'Bag Of Word (Deep clean)'\n",
        "    }\n",
        "    \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX3Uel5qZcFb"
      },
      "outputs": [],
      "source": [
        "ks_selected_df['label'].value_counts().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAgeYfVyaPuA"
      },
      "outputs": [],
      "source": [
        "is_manual = True\n",
        "\n",
        "# loop data df\n",
        "for i in task:\n",
        "\n",
        "  col = task[i]['col']\n",
        "  X = list(task[i]['data'][col])\n",
        "  y = list(task[i]['data']['label'])\n",
        "\n",
        "  X_test = list(ks_df_test[col])\n",
        "  y_test = list(ks_df_test['label'])\n",
        "\n",
        "  max_num_iter = 1000\n",
        "\n",
        "  if is_manual:\n",
        "\n",
        "    logreg_model = LogisticRegression(max_iter=max_num_iter, random_state=0,multi_class='multinomial')\n",
        "    logreg_model.fit(X, y)\n",
        "    y_pred = logreg_model.predict(X_test)\n",
        "    print(task[i]['language_model'])\n",
        "    print(classification_report(y_pred, y_test, digits = 4))\n",
        "\n",
        "    print('\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}